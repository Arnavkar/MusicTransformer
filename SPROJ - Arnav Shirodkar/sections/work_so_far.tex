\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../imgs/}}}
\begin{document}
\thispagestyle{plain}
\newpage

\section{Current Progress}

\subsection{The Model Environment}

Given that our model is intended to be functioning within a real-time setting and with a real performer the model must be positioned within an environment that is both optimized to work in real-time and that can function with custom code input. Furthermore, our environment should be able to parse auditory streams to MIDI data in real time, which will be presented to the machine learning model as input events. Given that the ability to parse auditory information is not trivial but is not the main focus of the project, it will be built as a separate module that makes use of open-source code and tools as much as possible. Lastly, the environment must also have a scheme for how data is transferred to and fro between the model, including how frequently MIDI notes are passed to the model and how the environment converts the model's output back into sound. Lastly, setting up our environment should not be so difficult that a non-programmer would be unable to play with the system. 

Given these requirements, I have designed the model's ``environment'' in Max/MSP. Max/MSP, also known as Max, is a visual programming language for music and multimedia developed by Cycling '74. It provides a graphical interface for creating and manipulating audio, video, and other multimedia content and is widely used in the fields of music composition, live performance, sound design, and interactive multimedia art. It is easy to use, highly optimized as an audio processing unit, and has an assortment of pre-built modules that directly address our needs, like a pitch-tracking module. 

Furthermore, Max/MSP also features an internal Node.js runtime engine that can run and interact with custom Javascript scripts. With the full flexibility of Javascript at our disposal within Max/MSP, I have opted to use the Node.js runtime to create an Improvisor class that houses our transformer model and serves as the interface between our model and Max/MSP. All custom is currently being written in Typescript before being compiled into Javascript.

\subsection{Prototype for the Model Environment}


\section{Building the model - Where I am Currently}

Given my choice to work with Max/MSP, the trained model must be able to be run independently of a specific programming language. To this end, I have chosen Tensorflow as my Machine learning library of choice, as it already has a Javascript port, \textit{Tensorflow.js}, that can run models built and compiled with the regular Tensorflow package and Python.

So far I have implemented a working baseline transformer from scratch in Tensorflow by subclassing the Layer and Model objects provided by Keras to create the Encoder and Decoder layers. I have also manually implemented the mathematical operations for scaled dot product attention in the baseline transformer. I am currently evaluating the transformer on a baseline language translation problem to verify that my code works as expected before modifying it to the Music domain. 

Within the Music Domain, I am currently working with the Maestro Dataset \cite{maestro}, which contains about 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. Audio and MIDI files are aligned with 3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. While this dataset is incredibly granular and well documented, the data consists largely of classical music. Within the realm of improvisation, it makes more intuitive sense to consider MIDI datasets for Jazz, which I am currently exploring given that most of these files are in the Lead Sheet format (which is what Improv RNN was trained on!)

I have also found a working sample of the midi-encoding proposed by Oore et al. for the Maestro Dataset and have written a custom Dataset class that will first encode the Maestro Dataset, then feed the encoded midi training examples to the Transformer in a custom training loop.

Next steps include:

\begin{enumerate}
    \item Verifying that my baseline transformer works as expected for the Language Translation Task

    \item Modifying the input layers to instead receive the encoded Midi values and evaluate the midi output from a trained baseline transformer against a pre-selected set of melodies

    \item Modify the attention mechanism in the Transformer to use the relative attention mechanism
    
    \item Train the modified Transformer and evaluate the midi-output against a pre-selected set of melodies

    \item Compile and export the trained modified transformer model and test its ability to run within the node.js runtime environment within Max, purely using inference.
\end{enumerate}

\end{document}