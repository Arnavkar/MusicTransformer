\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../imgs/}}}
\begin{document}
\newpage

\chapter{Introduction}

With the widespread proliferation of AI technology, deep architectures — many of which are based on neural networks — have advanced the state of the art in a variety of different research areas and applications. Within the relatively new domain of Music Information Retrieval (MIR), deep neural networks have also been successful for a variety of tasks, including tempo estimation, beat detection, genre classification, etc. {CITE}

Drawing inspiration from projects like George E. Lewis's \textit{Voyager}\cite{Lewis:1} and Al Biles's \textit{GenJam}\cite{Biles:1} two pioneering endeavors in human-computer interaction, this research proposes the use of transformer models to create a real-time musical improvisation companion and demonstrate the potential of AI to enhance the human experience of music through interdisciplinary exploration. The Transformer, a groundbreaking model introduced in the 2017 paper "Attention is All You Need" by Vaswani et. al, \cite{Vaswani:1} is at the core of Large Language Models (LLMs) like ChatGPT that have exploded in popularity around the world. We hope to use a transformer architecture 

This paper discusses the historical context for the rise of generative AI technology, popular model architectures for sequence-to-sequence tasks that are can be directly compared to our own and dives deep into the details of the transformer architecture. The paper also discusses the introduction of deep architectures into the budding field of MIR as well as prior deep-learning approaches in Music Generation.
 
The paper then evaluates personal attempts to build a transformer model for the task of musical improvisation, documenting various challenges and design decisions along the way as well as evaluating the model(s) through a variety of means. The design for the model's inference interface is also discussed, as well as opportunities for future improvement and work.

\end{document}
