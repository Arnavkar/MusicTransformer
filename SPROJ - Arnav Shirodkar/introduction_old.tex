\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../imgs/}}}
\begin{document}
\newpage
\section{Introduction}

With the rise of the technology sector, the field of computer science has seen its reach extend exponentially. Today, computing involves a vast mixture of interdisciplinary approaches that aim to automate existing processes or invent new, more efficient processes that assist us in our everyday life. Computers have already become ubiquitous in most of our lives; even your refrigerator may have some kind of smart chip that helps automate its internal processes. It is no surprise then, that AI has found its way into the arts. In the visual arts for example, models like Dall-E 2 have become incredibly popular. The same, however, is not necessarily true in the still budding field of Music Information Retrieval (MIR).

\subsection{Deep Learning in Music Information Retrieval}

As an attempt to analyze, retrieve and organize music, the field of Music Information Retrieval has emerged mostly in the last two decades. It has grown since its early beginnings to encompass a diverse collection of topics, including music perception, cognition, generation, and information retrieval. It borders on several other well-established fields, such as psychology, neuroscience, musicology, and computer science.

As in many other fields of research, a significant increase in algorithm accuracy and efficiency has been achieved in recent years for MIR tasks, due to the introduction of deep neural networks to the field. In its broad definition, a deep learning algorithm constructs multiple levels of data abstraction (a hierarchy of features) in order to model high-level representations present in the observed data. Such neural networks can often have thousands to millions of trainable parameters to cover the target domain, creating the need for very large training data-sets. In the early years of MIR, this was a huge problem, as there were few publicly available datasets that were cited and labelled explicitly for supervised Machine Learning tasks. Today, there are far more  labelled datasets in different representations (Raw Audio, Audio Stems, MIDI Data etc.) that provide us data to train MIR models, but not nearly as much as in other domains Image Processing or Natural Language Processing.


\subsection{Working with Transformers}
One major issue with neural-network architectures is that after high-level representations in the training data are encoded in a multi-layer hierarchy, the encoded knowledge is implicit and difficult to explain in a transparent way. There have been many attempts to visualize these learned concepts, but we are still far from being able to explain the data encoded into neural networks (hence the term “black box”) \cite{Bilal_et_al:1}. In the field of image processing, where convolutional neural networks have found immense success, such attempts include deconvolution, input occlusion or even training a separate network to invert feature representation and predict expected image input. \cite{Dosovitskiy_Brox:1,Zeiler_Fergus:1}  In the case of Music Information Retrieval, the inability to interpret the learned concepts of a neural network make it hard for us to improve upon it, owing to the fact listening to music is an extremely subjective experience for people. However, recent work with Transformers


% \vspace{0.5cm}

% Deep neural networks also have a large number of trainable parameters to cover the entire target domain, which creates the need for very large training data-sets. This is a major issue within the field of MIR. First, large data-sets may be difficult to acquire due to the scarcity of the appropriate data, potential copy-right issues, or pure storage requirements (High quality or lossless audio files are especially large. Another drawback — that is arguably more severe — is that in most cases, training data-sets need to be annotated for supervised learning. Furthermore, the annotations are most commonly subjective to the listener (e.g. genre classification in music) and therefore require either multiple annotators to approximate the human perception of the problem, or often require an expert, both of which require a significant amount of time and manpower. I experienced this first hand while working at the music AI startup Musiio. We meticulously prepared thousands of tracks across multiple playlists as a genre classification training data set. A huge amount of time is spent finding tracks, putting them into playlists and vetting said tracks, including plenty of debate with others about where tracks with multiple genre influences should be placed.

% \vspace{0.5cm}

% While deep neural architectures have been highly successful for a variety of classification tasks, such a model may struggle without labelled input, instead producing its own observations of high-level abstractions that are not necessarily pertinent to the required task. A desirable feature of a deep model is that it should also be to provide a set of responses to unfamiliar input, and if there is more than one explanation of a given input, the model’s output should provide several alternative explanations, as well as their likelihoods. For all of the collective reasons above, I found myself drawn away from traditional deep architectures, toward the Compositional Hierarchical Network implemented by M.Pesek, which aims to solve many of these problems. 

\subsection{Hierarchical Models}

The main principle of hierarchical models lies in the hierarchical nature of our perception of the world. Just like the human visual system can discern complex forms by combining basic elements, such as edges, lines, contrasts, and colors into increasingly more complex shapes and contours, so can the human auditory system group frequency components into auditory events, multiple tonal events into harmonies and their time evolution into melodies and harmonic progressions. The compositional hierarchical structures are therefore intuitively similar to our conscious perception. The model proposed by M.Pesek has its roots in the field of computer vision and is inspird by the Learned Hierarchy Of Parts (LHOP) model, described in greater detail below.

\subsubsection{Hierarchical Models In Computer Vision}
\noindent
\textbf{Hierarchical Compositional Network (HCN)} — George et al.\cite{Gredilla_Liu_Phoenix_George:1} recently presented the Hierarchical Compositional Network (HCN). The approach is a directed generative model able to ``discover and disentangle, without supervision, the building blocks of a set of binary images". The model is composed of binary features which are defined hierarchically as a composition. The compositions are formed from the features on the layer below and form compositions on the consecutive layers. To achieve transparency, the authors proposed new inference and learning processes. They introduced max-product message passing (MPMP), a significantly extended approach to the well-known sum-product message passing. According to the authors, the MPMP ``can learn features that are composable, interpretable, and causally meaningful". The features can be employed for image reconstruction and therefore give researchers a tangible insight into the model.

\vspace{0.5cm}

\noindent
\textbf{Learned Hierarchy of Parts (LHOP)} — In 2007, Leonardis and Fidler \cite{Fidler:1} presented a statistical approach to learning a hierarchy of parts in the field of computer vision.  They proposed a novel approach to constructing a hierarchical representation of visual input which aims to enable recognition and detection of a large number of object categories. Their approach is statistically driven and inspired by several principles: efficient indexing, robust matching and compositionality. Their idea is driven by the need for robust and flexible representations which they denote as ``parts", as shown in \textit{Figure \ref{fig:fig1}}. The principle of compositionality is employed throughout the model’s structure; by merging statistically significant features, new layers of compositions (parts composed of parts) are built. Although the model is used for object categorization, the lower layers are learned in a category-independent way to ``obtain complex, yet shareable visual building blocks, which is a crucial step towards a scalable representation". This model is the key inspiration for the model proposed by M.Pesek, as a generalized backbone structure which is able to extract "parts" uniquely based on the kind of input provided to the model.


\begin{figure}[hp]
    \centering
    \includegraphics[width=0.6\textwidth,height=10cm]{imgs/lhop.png}
    \caption{Representation of the LHOP model. Bottom layers learn in an unsupervised manner from upper layers, while the top most layer learns in a supervised manner.}
    \label{fig:fig1}
\end{figure}

The LHOP model was also further developed using a Histogram of Compositions\footnote[1]{
The Histogram of Compositions descriptor is based on the ``The histogram of oriented gradients" (HOG) a feature descriptor commonly used in computer vision and image processing for the purpose of object detection.} (HoC) descriptor \cite{Tabernik:1}. The HoC descriptor uses the generative model of hierarchical compositions for feature extraction and performs a hypothesis verification of detections produced by the hierarchical compositional model. Tabernik et al. evaluated the proposed descriptor and demonstrated its superiority in robustness on significantly occluded objects in comparison to the state-of-the-art convolutional neural network.

\subsubsection{Hierarchical Models in MIR}

As mentioned earlier, the choice of a hierarchical model for MIR tasks is rather intuitive when we consider the hierarchical structures of music, both in the spectral and temporal sense. One common approach employed for hierarchical models is to construct them as rule-based systems based on music theory but these are often limited to specific kinds of music, and in their ability to be automated. Two alternative hierarchical models have been proposed, the Multiple Viewpoint System and the Information Dynamics of Thinking architecture, which are discussed below. 

\textbf{Rule Based Systems} — One such rule-based, hierarchical approach to music theory is the technique of Schenkerian analysis, proposed by Heinrich Schenker. \cite{Schenker} This analysis, while done by hand, has been incredibly influential in its attempts to unveil fundamental underlying structures in various pieces of music, especially within the canon of German Romantic Music. Despite it's usefulness, such rule based systems are rarely suitable on all music, for their expert analysis is tailored to specific genres of music. For example, fundamental assumptions held in Schenkerian analysis make it an notoriously poor choice for the analysis of any music that falls outside of it's purview. Furthermore, a rule-based system like Schenkerian analysis can be very difficult to automate. Marsden, \cite{Marsden:1} proposed a system for automatically deriving the Schenkerian reductions of extracts of tonal extract. While the implementation was successful, it required huge amounts of space and energy, and returned many different results which differed in quality and correctness, making it unsuitable for use as a real-world analysis tool.

\textbf{Multiple Viewpoint System} — Conklin and Anagnostopoulou \cite{Conklin:2} devised a multiple viewpoint pattern discovery algorithm via suffix-trees\footnote[2]{The suffix tree is a data structure that stores all the suffixes of a given string in a compact tree-based structure. Its design allows for a particularly fast implementation of many important string operations.}. For a selected viewpoint, a specific transformation of a musical event into an abstract feature, the algorithm builds a suffix tree of viewpoint sequences, which are essentially transformed excerpts of music. After selecting the patterns that meet the specified frequency and significance thresholds, the leafs of the suffix tree are reported as the longest significant patterns in the corpus. 

Conklin also presents two algorithms based on viewpoints for statistical modeling of the melody \cite{Conklin:1} Given that a viewpoint is a function which computes values for the events in a sequence; a pattern is a sequence of such feature sets, where the latter represent a logical conjunction of multiple viewpoints. The authors present a complete algorithm which can find all ’maximal frequent patterns’ and an optimization algorithm using a faster heuristic approach, where the found patterns may not always be the maximal frequent patterns. The maximal frequent pattern represents a pattern whose component feature set cannot be further specialized without the pattern becoming infrequent.

\textbf{Information Dynamics of thinking architecture} — Wiggins and Forth described a cognitive architecture that is closer to the model proposed by Pesek and named it the Information Dynamics of Thinking (IDyOT) \cite{Wiggins:1}. The architecture is a step towards a model which includes ”aspects of human creativity and other forms of cognitive processing in terms of a preconscious predictive loop”. It is a hierarchical architecture that includes a number of generators on the first layer, employed to sample the input. Each generator produces an output distribution based on the input sequence. The architecture attempts to model a cognitive cycle, based on the statistical observations of input sequences. The the output generated are perceptual objects, such as pitch, timbre, amplitude and time. The generators’ predictions are formed into chunks based on the selection.

Predictions, which match the perceptual input, are grouped into sequences. If a sequence matches the information profile, a chunk is detected. The generator stores the chunk which is then included in the statistical model. This dynamic aspect results in an incremental learning process, which is different from the model proposed by Pesek. This model seems to address several limitations discussed in Pesek's dissertation, but there is no available implementation or published results of this system being applied to MIR tasks. For the purpose of real-time pattern recognition, the above model may provide the basis for modifying the model proposed by Pesek. 

\subsection{The Pattern Discovery Task in MIR}

The discovery of repeated patterns is a known problem in various other domains, including computer vision and bioinformatics. While it is common, the definition of "pattern discovery" fundamentally differs across each field, necessitating a need for unique algorithms and models within each domain. In music, the importance of repetition and variance have been discussed by a wide number of music theorists and researchers who have attempted to automate music analysis as much as possible (Eg. Marsden).  The task may also seem similar to the well-known pattern matching task, with some key differences. A pattern matching algorithm aims to find the location of a pattern within a dataset and usually has a clear quantitative relation between a query and a match. In comparison, a \textit{discovery} of patterns finds locations of multiple similar sequences of data in the dataset, without any prior information about the searched pattern. 

The Music Information Retrieval Exchange (MIREX) task definition of the \textit{discovery of repeated themes and sections} task from 2018 states that ”the algorithms take a piece of music as input, and outputs a list of patterns repeated within that piece”. While this is the task that Pesek's model was originally evaluated on, the latest version of the task from the MIREX 2021 is defined as follows:

\begin{displayquote}
\textbf{Subtask 1} — The Explicit Task - Algorithms that take an excerpt of music as input (the prime), and output a predicted continuation of the excerpt\\
\\\textbf{Subtask 2} — The Implicit Task - Algorithms that are given a prime and a candidate continuation and return the probability that this is the true continuation of the provided prime (i.e. the notes which occur immediately after)
\end{displayquote}

The tasks in this version of the task in the 2021 MIREX is directly relevant to my goal for the intended model, as it should have the ability to respond to musical audio input in a meaningful way, while constructing learned representations of the input in realtime. Furthermore, it will be useful to evaluate the results of models created for this later version of the MIREX pattern discovery task
\end{document}